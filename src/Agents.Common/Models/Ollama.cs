using System.Text;
using Agents.Common.Interfaces;
using Microsoft.Extensions.AI;

namespace Agents.Common.Models;

/// <summary>
/// Represents a language model interface using the LLm client for handling chat-based interactions.
/// This class allows interaction with a language model by sending prompts and receiving responses.
/// </summary>
public partial class Ollama : IFoundationModel
{
    private IChatClient Client { get; init; }
    private readonly List<ChatMessage> _chatHistory = [];
    private readonly bool _preserveHistory;

    private Ollama(IChatClient client, bool preserveHistory = true)
    {
        Client = client;
        _preserveHistory = preserveHistory;
    }

    /// <summary>
    /// Sends a prompt to the language model and retrieves a response.
    /// The prompt is added to the chat history and the response is generated by the language model client.
    /// </summary>
    /// <param name="prompt">The user input sent to the language model as a string.</param>
    /// <returns>A task representing the asynchronous operation. The result contains the response from the language model as a string, or null if no response is generated.</returns>
    public async Task<string?> SendMessage(string prompt)
    {
        if (!_preserveHistory) _chatHistory.Clear();
        
        _chatHistory.Add(new (ChatRole.User, prompt));
        var response = "";
        await foreach (var item in Client.GetStreamingResponseAsync(_chatHistory))
        {
            var bytes = Encoding.UTF8.GetBytes(item.Text);
            response +=  Encoding.UTF8.GetString(bytes);
        }
        _chatHistory.Add(new (ChatRole.Assistant, response));
        
        return response;
    }
}

/// <summary>
/// Provides a client implementation for interacting with a language model.
/// This class includes support for creating clients with specific configurations, such as custom endpoints or default models.
/// </summary>
public partial class Ollama
{
    public static IFoundationModel Create(string? endpoint = null, string? model = null, bool preserveHistory = true)
    {
        var client = new OllamaChatClient(endpoint ?? "http://localhost:11434", model ?? "llama3");
        return new Ollama(client, preserveHistory);
    }
}
