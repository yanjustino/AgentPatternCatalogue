using System.Text;
using Agents.Common.Interfaces;
using Agents.Common.Models;
using Microsoft.Extensions.AI;

namespace Agents.Common;

/// <summary>
/// Represents a language model interface using the LLm client for handling chat-based interactions.
/// This class allows interaction with a language model by sending prompts and receiving responses.
/// </summary>
public partial class LLmClient : ILLmClient
{
    private IChatClient Client { get; init; }
    private readonly List<ChatMessage> _chatHistory = [];
    private readonly bool _preserveHistory;

    private LLmClient(IChatClient client, bool preserveHistory = true)
    {
        Client = client;
        _preserveHistory = preserveHistory;
    }

    /// <summary>
    /// Sends a prompt to the language model and retrieves a response.
    /// The prompt is added to the chat history and the response is generated by the language model client.
    /// </summary>
    /// <param name="prompt">The user input sent to the language model as a string.</param>
    /// <param name="preserveHistory"></param>
    /// <returns>A task representing the asynchronous operation. The result contains the response from the language model as a string, or null if no response is generated.</returns>
    public async Task<string?> SendMessage(string prompt)
    {
        if (!_preserveHistory) _chatHistory.Clear();
        
        _chatHistory.Add(new (ChatRole.User, prompt));
        var response = "";
        await foreach (var item in Client.GetStreamingResponseAsync(_chatHistory))
        {
            var bytes = Encoding.UTF8.GetBytes(item.Text);
            response +=  Encoding.UTF8.GetString(bytes);
        }
        _chatHistory.Add(new (ChatRole.Assistant, response));
        
        return response;
    }

    /// <summary>
    /// Generates a plan by breaking a given request into a main task and its subtasks.
    /// This method sends a structured prompt to the language model and parses the response into a plan with a task description and a list of subtasks.
    /// </summary>
    /// <param name="input">The user input describing the request to be broken into a task and subtasks.</param>
    /// <returns>A task representing the asynchronous operation. The result contains a plan object with a main task description and subtasks, or null if no valid response is generated.</returns>
    public async Task<Plan?> Generate(string input)
    {
        var prompt = $"""
                      <context>
                        You are a planning assistant. Break the following request into a task and subtasks.
                      </context>  
                      <instruction>
                        Request: "{input}"
                      </instruction>
                      <output>
                        Task: [main task]
                        Subtasks:
                          - [step 1]
                          - [step 2]
                      </output>
                      """;
        
        var result = await SendMessage(prompt);

        var plan = new Plan();
        foreach (var line in result?.Split('\n') ?? [])
        {
            if (line.StartsWith("Task:")) plan.TaskDescription = line["Task:".Length..].Trim();
            else if (line.StartsWith("- ")) plan.Subtasks.Add(line["- ".Length..].Trim());
        }

        return string.IsNullOrWhiteSpace(plan.TaskDescription) ? null : plan;
    }    
}

/// <summary>
/// Provides a client implementation for interacting with a language model.
/// This class includes support for creating clients with specific configurations, such as custom endpoints or default models.
/// </summary>
public partial class LLmClient
{
    public static ILLmClient Create(string? endpoint = null, string? model = null, bool preserveHistory = true)
    {
        var client = new OllamaChatClient(endpoint ?? "http://localhost:11434", model ?? "llama3");
        return new LLmClient(client, preserveHistory);
    }
}
