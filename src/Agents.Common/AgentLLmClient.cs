using Agents.Common.Interfaces;
using Microsoft.Extensions.AI;

namespace Agents.Common;

/// <summary>
/// Represents a language model interface using the LLm client for handling chat-based interactions.
/// This class allows interaction with a language model by sending prompts and receiving responses.
/// </summary>
public partial class AgentLLmClient : IAgentLLmClient
{
    private IChatClient? Client { get; init; }
    private readonly List<ChatMessage> _chatHistory = [];

    /// <summary>
    /// Sends a prompt to the language model and retrieves a response.
    /// The prompt is added to the chat history and the response is generated by the language model client.
    /// </summary>
    /// <param name="prompt">The user input sent to the language model as a string.</param>
    /// <returns>A task representing the asynchronous operation. The result contains the response from the language model as a string, or null if no response is generated.</returns>
    public async Task<string?> SendMessage(string prompt)
    {
        _chatHistory.Add(new ChatMessage(ChatRole.User, prompt));
        var response = "";
        await foreach (var item in Client.GetStreamingResponseAsync(_chatHistory))
        {
            response += item;
        }

        _chatHistory.Add(new ChatMessage(ChatRole.Assistant, response));
        return response;
    }
}

/// <summary>
/// Provides a client implementation for interacting with a language model.
/// This class includes support for creating clients with specific configurations, such as custom endpoints or default models.
/// </summary>
public partial class AgentLLmClient
{
    public static IAgentLLmClient Create(IChatClient? chatClient) => new AgentLLmClient
    {
        Client = chatClient
    };

    public static IAgentLLmClient Create(string? endpoint = null, string? model = null) => new AgentLLmClient
    {
        Client = new OllamaChatClient(endpoint ?? "http://localhost:11434", model ?? "llama3")
    };

    public static IAgentLLmClient CreateDefault(string? model = null) => new AgentLLmClient
    {
        Client = new OllamaChatClient("http://localhost:11434", model ?? "llama3")
    };
}
